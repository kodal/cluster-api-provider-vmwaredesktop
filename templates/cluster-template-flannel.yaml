---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}
  labels:
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
spec:
  clusterNetwork:
    pods:
      cidrBlocks: ["10.244.0.0/16"]
  controlPlaneRef:
    kind: KubeadmControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    name: "${CLUSTER_NAME}-control-plane"
    namespace: ${NAMESPACE}
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
    kind: VDCluster
    name: "${CLUSTER_NAME}"
    namespace: ${NAMESPACE}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
kind: VDCluster
metadata:
  name: "${CLUSTER_NAME}"
  namespace: ${NAMESPACE}
spec:
  controlPlaneEndpoint:
    host: ${CONTROL_PLANE_ENDPOINT_IP}
    port: 6443
---
apiVersion: ipam.cluster.x-k8s.io/v1alpha1
kind: InClusterIPPool
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}
  labels:
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
spec:
  addresses:
    - 10.0.100.10-10.0.100.255
  prefix: 16
  gateway: 10.0.0.2
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
kind: VDMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-control-plane"
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      templateID: ${TEMPLATE_VM_ID}
      network:
        ethernets:
          - name: ens160
            dhcp4: true
          - name: ens224
            dhcp4: false
            nameservers:
              - 1.1.1.1
              - 8.8.8.8
            ipamRef:
              apiGroup: ipam.cluster.x-k8s.io
              kind: InClusterIPPool
              name: ${CLUSTER_NAME}
---
kind: KubeadmControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: "${CLUSTER_NAME}-control-plane"
  namespace: ${NAMESPACE}
spec:
  version: "${KUBERNETES_VERSION}"
  replicas: ${CONTROL_PLANE_MACHINE_COUNT=1}
  machineTemplate:
    infrastructureRef:
      kind: VDMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
      name: "${CLUSTER_NAME}-control-plane"
      namespace: ${NAMESPACE}
  kubeadmConfigSpec:
    initConfiguration:
      nodeRegistration:
        name: "{{ ds.meta_data.local_hostname }}"
        kubeletExtraArgs:
          provider-id: "vmwaredesktop://'{{ ds.meta_data.instance_id }}'"
    joinConfiguration:
      nodeRegistration:
        name: "{{ ds.meta_data.local_hostname }}"
        kubeletExtraArgs:
          provider-id: "vmwaredesktop://'{{ ds.meta_data.instance_id }}'"
    users:
      - name: root
        sshAuthorizedKeys:
          - ${VM_SSH_KEYS=" "}

    files:
      - content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            name: kube-vip
            namespace: kube-system
          spec:
            containers:
            - args:
              - manager
              env:
              - name: cp_enable
                value: "true"
              - name: vip_interface
                value: ${VIP_NETWORK_INTERFACE=""}
              - name: address
                value: ${CONTROL_PLANE_ENDPOINT_IP}
              - name: port
                value: "6443"
              - name: vip_arp
                value: "true"
              - name: vip_leaderelection
                value: "true"
              - name: vip_leaseduration
                value: "15"
              - name: vip_renewdeadline
                value: "10"
              - name: vip_retryperiod
                value: "2"
              image: ghcr.io/kube-vip/kube-vip:${KUBE_VIP_VERSION=v0.8.10}
              imagePullPolicy: IfNotPresent
              name: kube-vip
              resources: {}
              securityContext:
                capabilities:
                  add:
                  - NET_ADMIN
                  - NET_RAW
              volumeMounts:
              - mountPath: /etc/kubernetes/admin.conf
                name: kubeconfig
            hostAliases:
            - hostnames:
              - localhost
              - kubernetes
              ip: 127.0.0.1
            hostNetwork: true
            volumes:
            - hostPath:
                path: /etc/kubernetes/admin.conf
                type: FileOrCreate
              name: kubeconfig
          status: {}
        owner: root:root
        path: /etc/kubernetes/manifests/kube-vip.yaml
      - path: /etc/kube-vip-prepare.sh
        content: |
          #!/bin/bash

          # Copyright 2020 The Kubernetes Authors.
          #
          # Licensed under the Apache License, Version 2.0 (the "License");
          # you may not use this file except in compliance with the License.
          # You may obtain a copy of the License at
          #
          #     http://www.apache.org/licenses/LICENSE-2.0
          #
          # Unless required by applicable law or agreed to in writing, software
          # distributed under the License is distributed on an "AS IS" BASIS,
          # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
          # See the License for the specific language governing permissions and
          # limitations under the License.

          set -e

          # Configure the workaround required for kubeadm init with kube-vip:
          # xref: https://github.com/kube-vip/kube-vip/issues/684

          # Nothing to do for kubernetes < v1.29
          KUBEADM_MINOR="$(kubeadm version -o short | cut -d '.' -f 2)"
          if [[ "$KUBEADM_MINOR" -lt "29" ]]; then
            exit 0
          fi

          IS_KUBEADM_INIT="false"

          # cloud-init kubeadm init
          if [[ -f /run/kubeadm/kubeadm.yaml ]]; then
            IS_KUBEADM_INIT="true"
          fi

          # ignition kubeadm init
          if [[ -f /etc/kubeadm.sh ]] && grep -q -e "kubeadm init" /etc/kubeadm.sh; then
            IS_KUBEADM_INIT="true"
          fi

          if [[ "$IS_KUBEADM_INIT" == "true" ]]; then
            sed -i 's#path: /etc/kubernetes/admin.conf#path: /etc/kubernetes/super-admin.conf#' \
              /etc/kubernetes/manifests/kube-vip.yaml
          fi
        owner: root:root
        permissions: "0700"
      - path: /etc/systemd/system/mnt-hgfs.mount
        content: |
          [Unit]
          Description=VMware mount for hgfs
          DefaultDependencies=no
          Before=umount.target
          ConditionVirtualization=vmware
          After=sys-fs-fuse-connections.mount

          [Mount]
          What=vmhgfs-fuse
          Where=/mnt/hgfs
          Type=fuse
          Options=default_permissions,allow_other

          [Install]
          WantedBy=multi-user.target
      - path: /etc/modules-load.d/open-vm-tools.conf
        content: fuse
      - path: /etc/select-node-ip.sh
        owner: root:root
        permissions: "0700"
        content: |
          #!/bin/bash

          if [ -z "${NODE_SUBNET}" ]; then
            echo "NODE_SUBNET not set, skipping node-ip selection"
            exit 0
          fi

          ALLOWED4=("${NODE_SUBNET}")
          ALLOWED6=()

          IP4_LIST=($(ip -4 -o addr show scope global | awk '{print $4}' | cut -d/ -f1))
          IP6_LIST=($(ip -6 -o addr show scope global | awk '{print $4}' | cut -d/ -f1))

          select_ip=""
          ip_in_subnet() {
            local ip=$$1
            local cidr=$$2

            IFS=. read -r i1 i2 i3 i4 <<< "$$ip"
            IFS=/ read -r net mask <<< "$$cidr"
            IFS=. read -r n1 n2 n3 n4 <<< "$$net"

            ip_num=$(( (i1<<24) + (i2<<16) + (i3<<8) + i4 ))
            net_num=$(( (n1<<24) + (n2<<16) + (n3<<8) + n4 ))
            mask_num=$(( 0xFFFFFFFF << (32 - mask) & 0xFFFFFFFF ))

            if (( (ip_num & mask_num) == (net_num & mask_num) )); then
                return 0  # IP в сети
            else
                return 1  # IP не в сети
            fi
          }

          for ip in "$${IP4_LIST[@]}"; do
            for net in "$${ALLOWED4[@]}"; do
              if ip_in_subnet "$$ip" "$net"; then
                select_ip="$$ip"
                break 2
              fi
            done
          done

          if [ -z "$$select_ip" ]; then
            for ip in "$${IP6_LIST[@]}"; do
              for net in "$${ALLOWED6[@]}"; do
                if ip_in_subnet "$ip" "$net"; then
                  select_ip="$$ip"
                  break 2
                fi
              done
            done
          fi

          if [ -z "$$select_ip" ]; then
            echo "No suitable node IP found" >&2
            exit 1
          fi

          mkdir -p /etc/kubelet

          echo "KUBELET_EXTRA_ARGS=--node-ip=$select_ip" > /etc/kubelet/node_ip.env
      - path: /etc/systemd/system/kubelet.service.d/20-node-ip.conf
        content: |
          [Service]
          ExecStartPre=/etc/select-node-ip.sh
          EnvironmentFile=-/etc/kubelet/node_ip.env

    preKubeadmCommands:
      - echo "root:Kodal123" | chpasswd
      - /etc/kube-vip-prepare.sh
      - systemctl enable --now mnt-hgfs.mount
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-workers
  namespace: ${NAMESPACE}
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT=0}
  selector:
    matchLabels:
  template:
    spec:
      clusterName: ${CLUSTER_NAME}
      version: ${KUBERNETES_VERSION}
      bootstrap:
        configRef:
          name: ${CLUSTER_NAME}-worker
          namespace: ${NAMESPACE}
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
      infrastructureRef:
        name: "${CLUSTER_NAME}-worker"
        apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
        kind: VDMachineTemplate
        namespace: ${NAMESPACE}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
kind: VDMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-worker
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      templateID: ${TEMPLATE_VM_ID}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-worker
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      users:
        - name: root
          sshAuthorizedKeys:
            - ${VM_SSH_KEYS=" "}
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            provider-id: "vmwaredesktop://'{{ ds.meta_data.instance_id }}'"
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  name: ${CLUSTER_NAME}-crs-flannel
  namespace: ${NAMESPACE}
spec:
  clusterSelector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  resources:
    - kind: ConfigMap
      name: ${CLUSTER_NAME}-flannel
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ${CLUSTER_NAME}-flannel
  namespace: ${NAMESPACE}
data:
  data: |
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      labels:
        k8s-app: flannel
      name: flannel
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      labels:
        k8s-app: flannel
      name: flannel
    rules:
    - apiGroups:
      - ""
      resources:
      - pods
      verbs:
      - get
    - apiGroups:
      - ""
      resources:
      - nodes
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - nodes/status
      verbs:
      - patch
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      labels:
        k8s-app: flannel
      name: flannel
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: flannel
    subjects:
    - kind: ServiceAccount
      name: flannel
      namespace: kube-system
    ---
    apiVersion: v1
    data:
      cni-conf.json: |
        {
          "name": "cbr0",
          "cniVersion": "0.3.1",
          "plugins": [
            {
              "type": "flannel",
              "delegate": {
                "hairpinMode": true,
                "isDefaultGateway": true
              }
            },
            {
              "type": "portmap",
              "capabilities": {
                "portMappings": true
              }
            }
          ]
        }
      net-conf.json: |
        {
          "Network": "10.244.0.0/16",
          "EnableNFTables": false,
          "Backend": {
            "Type": "vxlan"
          }
        }
    kind: ConfigMap
    metadata:
      labels:
        app: flannel
        k8s-app: flannel
        tier: node
      name: kube-flannel-cfg
      namespace: kube-system
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      labels:
        app: flannel
        k8s-app: flannel
        tier: node
      name: kube-flannel-ds
      namespace: kube-system
    spec:
      selector:
        matchLabels:
          app: flannel
          k8s-app: flannel
      template:
        metadata:
          labels:
            app: flannel
            k8s-app: flannel
            tier: node
        spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: kubernetes.io/os
                    operator: In
                    values:
                    - linux
          containers:
          - args:
            - --ip-masq
            - --kube-subnet-mgr
            command:
            - /opt/bin/flanneld
            env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: EVENT_QUEUE_DEPTH
              value: "5000"
            image: ghcr.io/flannel-io/flannel:${KUBE_FLANNEL_VERSION=v0.26.6}
            name: kube-flannel
            resources:
              requests:
                cpu: 100m
                memory: 50Mi
            securityContext:
              capabilities:
                add:
                - NET_ADMIN
                - NET_RAW
              privileged: false
            volumeMounts:
            - mountPath: /run/flannel
              name: run
            - mountPath: /etc/kube-flannel/
              name: flannel-cfg
            - mountPath: /run/xtables.lock
              name: xtables-lock
          hostNetwork: true
          initContainers:
          - args:
            - -f
            - /flannel
            - /opt/cni/bin/flannel
            command:
            - cp
            image: ghcr.io/flannel-io/flannel-cni-plugin:v1.6.2-flannel1
            name: install-cni-plugin
            volumeMounts:
            - mountPath: /opt/cni/bin
              name: cni-plugin
          - args:
            - -f
            - /etc/kube-flannel/cni-conf.json
            - /etc/cni/net.d/10-flannel.conflist
            command:
            - cp
            image: ghcr.io/flannel-io/flannel:${KUBE_FLANNEL_VERSION=v0.26.6}
            name: install-cni
            volumeMounts:
            - mountPath: /etc/cni/net.d
              name: cni
            - mountPath: /etc/kube-flannel/
              name: flannel-cfg
          priorityClassName: system-node-critical
          serviceAccountName: flannel
          tolerations:
          - effect: NoSchedule
            operator: Exists
          volumes:
          - hostPath:
              path: /run/flannel
            name: run
          - hostPath:
              path: /opt/cni/bin
            name: cni-plugin
          - hostPath:
              path: /etc/cni/net.d
            name: cni
          - configMap:
              name: kube-flannel-cfg
            name: flannel-cfg
          - hostPath:
              path: /run/xtables.lock
              type: FileOrCreate
            name: xtables-lock
